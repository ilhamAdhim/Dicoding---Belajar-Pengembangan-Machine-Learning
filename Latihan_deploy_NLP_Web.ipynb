{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latihan deploy NLP Web.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNszsZjFD5372q6o5s7tZzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilhamAdhim/Dicoding---Belajar-Pengembangan-Machine-Learning/blob/main/Latihan_deploy_NLP_Web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SDJfmcIvms6",
        "outputId": "01bf8a66-c01f-4476-cbaf-a5603b7c7b93"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-3.9.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 30 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 51 kB 2.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 61 kB 2.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.40.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.5.0)\n",
            "Installing collected packages: tensorflowjs\n",
            "Successfully installed tensorflowjs-3.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C-elZpjqYjM"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'milham'\n",
        "os.environ['KAGGLE_KEY'] = '983807a117cd7dc7ef8c100971c407ce'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfN-OfHQwxxJ",
        "outputId": "6f9ba92d-5cac-45d2-d7b6-e160f1407d41"
      },
      "source": [
        "\n",
        "!kaggle datasets download -d marklvl/sentiment-labelled-sentences-data-set"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sentiment-labelled-sentences-data-set.zip to /content\n",
            "\r  0% 0.00/326k [00:00<?, ?B/s]\n",
            "\r100% 326k/326k [00:00<00:00, 44.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfSahr-pw0t5"
      },
      "source": [
        "!unzip -q sentiment-labelled-sentences-data-set.zip -d ."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhwnN3UUw3ZB"
      },
      "source": [
        "## **LSTM**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJN8z-3gw6SZ",
        "outputId": "299dca2a-d295-4125-ccc1-0b1464835980"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df = pd.read_csv('/content/sentiment labelled sentences/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sKKvAomw8b4",
        "outputId": "c83dee4e-cd98-45d2-f40a-1b99f4ed1db7"
      },
      "source": [
        "\n",
        "df.head"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                               sentence  label\n",
              "0                             Wow... Loved this place.      1\n",
              "1                                   Crust is not good.      0\n",
              "2            Not tasty and the texture was just nasty.      0\n",
              "3    Stopped by during the late May bank holiday of...      1\n",
              "4    The selection on the menu was great and so wer...      1\n",
              "..                                                 ...    ...\n",
              "995  I think food should have flavor and texture an...      0\n",
              "996                           Appetite instantly gone.      0\n",
              "997  Overall I was not impressed and would not go b...      0\n",
              "998  The whole experience was underwhelming, and I ...      0\n",
              "999  Then, as if I hadn't wasted enough of my life ...      0\n",
              "\n",
              "[1000 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl4Z0LIaxQug"
      },
      "source": [
        "\n",
        "# convert to lowercase\n",
        "df['sentence'] = df['sentence'].str.lower()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVI2FW_hxUrZ",
        "outputId": "b7810925-e354-49b1-d148-bdb457861dc6"
      },
      "source": [
        "# remove stopwords\n",
        "\n",
        "#from nltk.corpus import stopwords #comment jika Error dan gunakan 2 sintaks dibawah\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CwcHL19GxW_h",
        "outputId": "51ed0d1f-7f61-4aad-d5cb-f648d353be83"
      },
      "source": [
        "stop = set(nltk.corpus.stopwords.words('english'))\n",
        "df['sentence'] = df['sentence'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow... loved place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0                                wow... loved place.      1\n",
              "1                                        crust good.      0\n",
              "2                               tasty texture nasty.      0\n",
              "3  stopped late may bank holiday rick steve recom...      1\n",
              "4                       selection menu great prices.      1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E14pThCVyJf5",
        "outputId": "d033ee69-d181-49d9-a5fe-ec45eb356acd"
      },
      "source": [
        "vocab_size = 2000\n",
        "oov_tok = \"<OOV>\"\n",
        "filt = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ' #remove symbols\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok, filters = filt)\n",
        "tokenizer.fit_on_texts(df['sentence'].values)\n",
        "\n",
        "word2index = tokenizer.word_index\n",
        "print(len(word2index))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKLwgldGyLjQ"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('word2index.json', 'w') as fp:\n",
        "    json.dump(word2index, fp)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foA1KaMGyNtB",
        "outputId": "e4342f77-520c-4f64-fd77-cda9fef6fc99"
      },
      "source": [
        "max_length =  max(len(values.split()) for i, values in enumerate(df['sentence']))\n",
        "max_length"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk8-GLLzyPrA",
        "outputId": "e8f47a81-dc89-4c44-92f1-41922bbaa35c"
      },
      "source": [
        "trunc_type='post'\n",
        "\n",
        "all_seq = tokenizer.texts_to_sequences(df['sentence'].values)\n",
        "all_padded = pad_sequences(all_seq, maxlen = max_length, padding = trunc_type)\n",
        "all_padded.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdGg3LhHySYg",
        "outputId": "1ea7c0e7-d2b6-4a18-b1c7-3ccaa1f4385c"
      },
      "source": [
        "\n",
        "# split train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = all_padded\n",
        "#y = pd.get_dummies(df['label'].values)\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#kalimat = df['sentence'].values\n",
        "#y = df['label'].values\n",
        "\n",
        "#kalimat_latih, kalimat_test, y_latih, y_test = train_test_split(kalimat, y, \n",
        "#                                                                test_size=0.2, random_state=1000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 18) (800,)\n",
            "(200, 18) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swN-hbkryUOY",
        "outputId": "6357eca4-8a48-42df-82b8-f2135ca3fee5"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim= vocab_size, output_dim=16, input_length= max_length),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 18, 16)            32000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                20736     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                1560      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 54,321\n",
            "Trainable params: 54,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31A5Bo2_yXag",
        "outputId": "004c1cb7-d50e-48c0-f005-701accaaef53"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "25/25 [==============================] - 3s 35ms/step - loss: 0.6944 - accuracy: 0.4913 - val_loss: 0.6937 - val_accuracy: 0.4800\n",
            "Epoch 2/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.6928 - val_accuracy: 0.6550\n",
            "Epoch 3/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.6550 - accuracy: 0.6825 - val_loss: 0.5831 - val_accuracy: 0.7050\n",
            "Epoch 4/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.3081 - accuracy: 0.8913 - val_loss: 0.6074 - val_accuracy: 0.7350\n",
            "Epoch 5/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.1337 - accuracy: 0.9588 - val_loss: 0.6644 - val_accuracy: 0.7550\n",
            "Epoch 6/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0620 - accuracy: 0.9837 - val_loss: 0.7227 - val_accuracy: 0.7750\n",
            "Epoch 7/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0383 - accuracy: 0.9937 - val_loss: 0.9013 - val_accuracy: 0.7550\n",
            "Epoch 8/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0250 - accuracy: 0.9950 - val_loss: 1.2843 - val_accuracy: 0.7350\n",
            "Epoch 9/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0324 - accuracy: 0.9912 - val_loss: 1.0811 - val_accuracy: 0.7500\n",
            "Epoch 10/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9937 - val_loss: 1.0041 - val_accuracy: 0.7550\n",
            "Epoch 11/30\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.0170 - accuracy: 0.9975 - val_loss: 1.0444 - val_accuracy: 0.7650\n",
            "Epoch 12/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0162 - accuracy: 0.9975 - val_loss: 1.1593 - val_accuracy: 0.7650\n",
            "Epoch 13/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 0.9975 - val_loss: 1.0459 - val_accuracy: 0.7700\n",
            "Epoch 14/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 1.2958 - val_accuracy: 0.7600\n",
            "Epoch 15/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9975 - val_loss: 1.4908 - val_accuracy: 0.7450\n",
            "Epoch 16/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0204 - accuracy: 0.9962 - val_loss: 1.1444 - val_accuracy: 0.7550\n",
            "Epoch 17/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0164 - accuracy: 0.9937 - val_loss: 1.2778 - val_accuracy: 0.7600\n",
            "Epoch 18/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 1.2552 - val_accuracy: 0.7600\n",
            "Epoch 19/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.9842 - val_accuracy: 0.7850\n",
            "Epoch 20/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 1.1273 - val_accuracy: 0.7650\n",
            "Epoch 21/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0027 - accuracy: 0.9987 - val_loss: 1.2068 - val_accuracy: 0.7600\n",
            "Epoch 22/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3796 - val_accuracy: 0.7750\n",
            "Epoch 23/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 3.7223e-04 - accuracy: 1.0000 - val_loss: 1.4580 - val_accuracy: 0.7550\n",
            "Epoch 24/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 3.0035e-04 - accuracy: 1.0000 - val_loss: 1.5035 - val_accuracy: 0.7550\n",
            "Epoch 25/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 2.2664e-04 - accuracy: 1.0000 - val_loss: 1.5571 - val_accuracy: 0.7600\n",
            "Epoch 26/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.9058e-04 - accuracy: 1.0000 - val_loss: 1.5871 - val_accuracy: 0.7600\n",
            "Epoch 27/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.6428e-04 - accuracy: 1.0000 - val_loss: 1.6156 - val_accuracy: 0.7600\n",
            "Epoch 28/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.4308e-04 - accuracy: 1.0000 - val_loss: 1.6403 - val_accuracy: 0.7600\n",
            "Epoch 29/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.2842e-04 - accuracy: 1.0000 - val_loss: 1.6628 - val_accuracy: 0.7600\n",
            "Epoch 30/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 1.1678e-04 - accuracy: 1.0000 - val_loss: 1.6849 - val_accuracy: 0.7600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm9c3NfLyZnZ"
      },
      "source": [
        "\n",
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeSSMxhGyb0A",
        "outputId": "c4ce69e8-2ce2-47cc-8fd5-69b33265114c"
      },
      "source": [
        "saved_model_path = '/content/mymodel/'\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/mymodel/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/mymodel/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW73oApuyf_g",
        "outputId": "5674ed5e-b61d-4725-a6e6-a2d6d744e839"
      },
      "source": [
        "!tensorflowjs_converter \\\n",
        "  --input_format=tf_saved_model \\\n",
        "  /content/mymodel/ \\\n",
        "  /content/modeltfjs"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-01 07:38:22.589350: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-10-01 07:38:22.589413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0393b3f6db26): /proc/driver/nvidia/version does not exist\n",
            "2021-10-01 07:38:24.660780: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2021-10-01 07:38:24.661037: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
            "2021-10-01 07:38:24.671631: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
            "  function_optimizer: Graph size after: 251 nodes (239), 348 edges (336), time = 5.186ms.\n",
            "  function_optimizer: function_optimizer did nothing. time = 0.09ms.\n",
            "\n",
            "2021-10-01 07:38:24.901269: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
            "  debug_stripper: debug_stripper did nothing. time = 0.031ms.\n",
            "  model_pruner: Graph size after: 163 nodes (-40), 211 edges (-40), time = 1.065ms.\n",
            "  constant_folding: Graph size after: 156 nodes (-7), 204 edges (-7), time = 5.615ms.\n",
            "  arithmetic_optimizer: Graph size after: 156 nodes (0), 204 edges (0), time = 1.887ms.\n",
            "  dependency_optimizer: Graph size after: 138 nodes (-18), 162 edges (-42), time = 1.111ms.\n",
            "  model_pruner: Graph size after: 138 nodes (0), 162 edges (0), time = 0.537ms.\n",
            "  constant_folding: Graph size after: 138 nodes (0), 162 edges (0), time = 2.402ms.\n",
            "  arithmetic_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 1.685ms.\n",
            "  dependency_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 0.804ms.\n",
            "  debug_stripper: debug_stripper did nothing. time = 0.073ms.\n",
            "  model_pruner: Graph size after: 138 nodes (0), 162 edges (0), time = 0.447ms.\n",
            "  constant_folding: Graph size after: 138 nodes (0), 162 edges (0), time = 1.996ms.\n",
            "  arithmetic_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 1.748ms.\n",
            "  dependency_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 0.801ms.\n",
            "  model_pruner: Graph size after: 138 nodes (0), 162 edges (0), time = 0.481ms.\n",
            "  constant_folding: Graph size after: 138 nodes (0), 162 edges (0), time = 1.967ms.\n",
            "  arithmetic_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 1.73ms.\n",
            "  dependency_optimizer: Graph size after: 138 nodes (0), 162 edges (0), time = 0.804ms.\n",
            "\n",
            "2021-10-01 07:38:24.923850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
            "  remapper: Graph size after: 135 nodes (-3), 159 edges (-3), time = 0.523ms.\n",
            "  constant_folding: Graph size after: 135 nodes (0), 159 edges (0), time = 2.786ms.\n",
            "  arithmetic_optimizer: Graph size after: 135 nodes (0), 159 edges (0), time = 1.806ms.\n",
            "  dependency_optimizer: Graph size after: 135 nodes (0), 159 edges (0), time = 0.86ms.\n",
            "  remapper: Graph size after: 135 nodes (0), 159 edges (0), time = 0.383ms.\n",
            "  constant_folding: Graph size after: 135 nodes (0), 159 edges (0), time = 2.23ms.\n",
            "  arithmetic_optimizer: Graph size after: 135 nodes (0), 159 edges (0), time = 1.889ms.\n",
            "  dependency_optimizer: Graph size after: 135 nodes (0), 159 edges (0), time = 0.936ms.\n",
            "\n",
            "Writing weight file /content/modeltfjs/model.json...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9poPsUl30Kk6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        " \n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['sentence'].values)\n",
        " \n",
        "word2index = tokenizer.word_index"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCupZy9B0Ke4"
      },
      "source": [
        "import json\n",
        "with open('word2index.json', 'w') as fp:\n",
        "    json.dump(word2index, fp)"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}